---
title: "Course Project Practical Machine learning"
author: "B vStraaten"
date: "February 7, 2018"
output: html_document
---

## Summary
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. 

One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, we will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.

We will attempt to predict the manner in which they did the exercise. If we manage to train a good model, we will use this model to predict 20 test cases.

Firstly, let's set up the enviroment.
```{r setup, message = FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Required packages
library(readr)
library(ggplot2)
library(caret)
library(dplyr)
library(kableExtra)
library(knitr)

# Get/ set Working directory
getwd()
```

## Load data
```{r load, message = FALSE, warning = FALSE}
pml_data <- read_csv("pml-training.csv", na = "NA")
pwl_predictions <- read_csv("pml-testing.csv", na = "NA")
```

## Inspecting data
```{r inspect}
cases_training <- dim(pml_data)[1]
cases_testing <- dim(pwl_predictions)[1]
kable(table(pml_data$classe, pml_data$user_name))
```

After inspecting the data we see `r cases_training` cases in the training set. From the table above we see 5 participants, and 5 different outcomes:

  * Exactly according to the specification (Class A)
  * Throwing the elbows to the front (Class B)
  * Lifting the dumbbell only halfway (Class C)
  * Lowering the dumbbell only halfway (Class D)
  * Throwing the hips to the front (Class E).

The test set has `r cases_testing` cases, which are the cases we will use for the prediction quiz.

Furthermore, after visually inpspecting the data we see 3 types of missing values: #DIV/0!, NA and empty cells. We will have te deal with this before we van move on.

## Transform data
The easiest way for me is to reload the data and mark #DIV/0!, NA and empty cells as "NA". I will name the set with 20 cases the 'predictions' data.
```{r reload, message = FALSE, warning = FALSE}
pml_data <- read_csv("pml-training.csv", na = c("NA", "#DIV/0!", ""))
pwl_predictions <- read_csv("pml-testing.csv", na = c("NA", "#DIV/0!", ""))
```

Now we have all of our missing values neately marked as "NA", let's count how many missings we have.
```{r missings}
## count NA's [1]
na_count <-sapply(pml_data, function(y) sum(length(which(is.na(y)))))
na_count <- data.frame(na_count)
na_count <- round(na_count / dim(pml_data)[1], digits=2)
```

The above code counts the missing values for each variable. We then divide this by total cases, so we end up with a percentage of missing values. See table 1 in the appendix for variables with 90% or more missings.
```{r clean}
na_count2 <- subset(na_count, na_count < 0.9)
pml_data2 <- select(pml_data, row.names(na_count2))

pml_data3 <- select(pml_data2, 
                    c(-X1, -raw_timestamp_part_1, -raw_timestamp_part_2, 
                      -cvtd_timestamp, -new_window, -num_window))

pml_data3 <- pml_data3[complete.cases(pml_data3), ]

cases_clean <- dim(pml_data3)[1]
vars_clean <- dim(pml_data3)[2]
```

We will now delete all the variabels with 90% or more missings. We will also delete some variables that do not seem relevant based on intution, like timestamps, windows etc. Finally, we delete all cases that still have missings with the complete.cases command. We end up with a clean dataset containing `r vars_clean` variables and `r cases_clean` cases.

## Model building
We set the seed for reproducibility
```{r seed}
set.seed(1234)
```

Now we will split the data in 60% training data, and 40% testing data.
```{r split_data}
inTrain <- createDataPartition(y = pml_data3$classe, p = 0.6, list = FALSE)
training <- pml_data3[inTrain, ]
testing <- pml_data3[-inTrain, ]
```

Since the outcome that we want to predict is a 5 level factor variable it seems logical to apply tree based models.

### Model 1: classification tree
Let us start with a simple classification tree. For straters, let's tell Caret to use a simple 5-fold cross-validation because it is considerable less time consuming that the 25 repeat bootstrap default.If we need more accuracy, we'll get back to this.

```{r model1, cache = TRUE}
fitControl <- trainControl(method = "cv", number = 5)

fit_tree <- train(classe ~ .,
                  method = "rpart",
                  data = training,
                  trControl = fitControl)
```

Now, let's check how model 1 performs on the test set. We use Caret's predict funtion to apply our model on the test data.
```{r performance2}
pred_tree <- predict(fit_tree, testing)
accuracy_tree <- round(confusionMatrix(pred_tree, testing$classe)$overall[1], digits = 2)
out_of_sample_e1 <- round(1 - accuracy_tree, digits = 2)
```
We plot a confucion matrix which shows us the predicted classe versus the observed classe in the test data. from there, it also gives us -amongst other performance indicators- the accuracy, wich is `r accuracy_tree`. See appendix for confusion matrix 1: classification tree.

The out of sample error is easely calculated from this accuracy metric: 1 - accuracy = `r out_of_sample_e1`. The expected error is to high, so we will try a random forest model next, hoping that a random forest model will perform better.

### Model 2: random forest
```{r model2, cache = TRUE}
fitControl2 <- trainControl(method = "cv", number = 5)

fit_rf <- train(classe ~ .,
                method = "rf",
                data = training,
                trControl = fitControl2)
```

Again, we use a simple 5-fold cross-validation because it is considerably less time consuming that the 25 repeat bootstrap default.
```{r performance4}
pred_rf <- predict(fit_rf, testing)
accuracy_RF <- round(confusionMatrix(pred_rf, testing$classe)$overall[1], digits = 2)
out_of_sample_e2 <- round(1 - accuracy_RF, digits = 2)
```
We plot a confucion matrix which shows us the predicted classe versus the observed classe in the test data. The accuracy of the random forest model is much higher: `r accuracy_RF`. Logically, the expected out of sample error is also much smaller: `r out_of_sample_e2`.

See appendix for confusion matrix 2: random forest.

## Predict cases

The random forest model has a high accuracy and therefore the expected out of sample error is low. Let's use this model to predict the 20 cases in the predictions data.
```{r prediction}
predictions <- predict(fit_rf, pwl_predictions)
predictions
```
Well, that's 20 out of 20 correct :).

## Appendix

### Table 1: variables with more than 90% missing values
```{r table1}
kable(subset(na_count, na_count > 0.9),  "html") %>%
  kable_styling() %>%
  scroll_box(height = "200px")
```

### Table 2: variables with or without near zero variance
```{r table2}
kable(nearZeroVar(pml_data3, saveMetrics= TRUE), "html") %>%
  kable_styling() %>%
  scroll_box(height = "200px")
```

### Confusion matrix 1: classification tree
```{r confusionmatrix1}
confusionMatrix(pred_tree, testing$classe)
```

### Confusion matrix 2: random forest
```{r confusionmatrix2}
confusionMatrix(pred_rf, testing$classe)
```

### Sources

  * http://groupware.les.inf.puc-rio.br/har
  * https://stackoverflow.com/questions/24027605/determine-the-number-of-na-values-in-a-column
  * https://topepo.github.io/caret/pre-processing.html#zero--and-near-zero-variance-predictors
  * https://stackoverflow.com/questions/24801452/error-in-confusionmatrix-the-data-and-reference-factors-must-have-the-same-numbe
  * http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/
